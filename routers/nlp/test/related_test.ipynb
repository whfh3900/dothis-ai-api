{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append(\"/app/routers/nlp/util\")\n",
    "sys.path.append(\"../nlp_util/\")\n",
    "\n",
    "from dothis_keyword import VBR, GensimRelated\n",
    "from dothis_nlp import decode_and_convert\n",
    "from ai_dataload import download_file_from_s3\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import json\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### to temp\n",
    "def vbr_data_collect():\n",
    "\n",
    "    def get_dates_between(start_date, end_date):\n",
    "        \"\"\"\n",
    "        두 날짜 사이의 모든 날짜를 리스트로 반환하되, 현재 날짜 이후의 날짜는 제외합니다.\n",
    "        \n",
    "        :param start_date: 시작 날짜 (YYYY-MM-DD 형식의 문자열)\n",
    "        :param end_date: 종료 날짜 (YYYY-MM-DD 형식의 문자열)\n",
    "        :return: 두 날짜 사이의 모든 날짜를 포함하는 리스트 (현재 날짜 이후 제외)\n",
    "        \"\"\"\n",
    "        # 현재 날짜\n",
    "        current_date = datetime.now().strftime('%Y%m%d')\n",
    "        \n",
    "        # 날짜 범위 생성\n",
    "        dates = pd.date_range(start=start_date, end=end_date)\n",
    "        \n",
    "        # 날짜를 문자열 형식으로 변환하여 리스트로 반환, 현재 날짜 이후는 제외\n",
    "        return [date.strftime('%Y%m%d') for date in dates if date.strftime('%Y%m%d') <= current_date]\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "    # PROXMOX MYSQL 데이터베이스 연결 정보\n",
    "    host = os.environ.get('MYSQL_HOST') # RDS 엔드포인트 URL 또는 IP 주소\n",
    "    port = int(os.environ.get('MYSQL_PORT')) # RDS 데이터베이스 포트 (기본값: 3306)\n",
    "    user = os.environ.get('MYSQL_USER') # MySQL 계정 아이디\n",
    "    password = os.environ.get('MYSQL_PW') # MySQL 계정 비밀번호\n",
    "    db_name = \"dothis_pre\" # 데이터베이스 이름\n",
    "\n",
    "    # PROXMOX MYSQL 연결하기\n",
    "    conn = pymysql.connect(host=host, \n",
    "                        port=port,\n",
    "                        user=user, \n",
    "                        password=password, \n",
    "                        db=db_name, \n",
    "                        charset='utf8mb4')\n",
    "\n",
    "    # 데이터베이스 커서(Cursor) 객체 생성\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    end_limit = 30\n",
    "    now_limit = 0\n",
    "    end_total = 7\n",
    "    now_total = 0\n",
    "    time_date = 0\n",
    "\n",
    "    while True:\n",
    "        if end_total == now_total:\n",
    "            break\n",
    "\n",
    "        ### time_date까지 데이터가 채워지지 않았을 경우를 대비.\n",
    "        if end_limit == now_limit:\n",
    "            print(f\"Data is not sufficiently populated. Collects only total {end_total} match data by {end_limit}.\")\n",
    "            # logger.info(f\"Data is not sufficiently populated. Collects only total {end_total} match data by {end_limit}.\")\n",
    "            break\n",
    "\n",
    "        start_date = datetime.now() - timedelta(days=time_date)\n",
    "        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "        _start_date_str = start_date.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        end_date = start_date + timedelta(days=7)\n",
    "        _end_date_str = end_date.strftime(\"%Y%m%d\")\n",
    "        end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "        dates = get_dates_between(start_date_str, end_date_str)\n",
    "        # print(\"start_date_str, end_date_str\", start_date_str, end_date_str)\n",
    "        # print(\"dates\", dates)\n",
    "        ############################ pre 데이터 가져오기\n",
    "        # 테이블 존재 여부 확인 쿼리\n",
    "        video_data_table = f\"video_data_{_start_date_str}\"\n",
    "        check_query = f\"SELECT table_name FROM information_schema.tables WHERE table_schema = '{db_name}' AND table_name = '{video_data_table}';\"\n",
    "        cursor.execute(check_query)\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            now_limit += 1\n",
    "            time_date += 1\n",
    "            continue\n",
    "        \n",
    "        query = f\"\"\"\n",
    "                desc {db_name}.{video_data_table};\n",
    "                \"\"\"\n",
    "        cursor.execute(query)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "                select * from {db_name}.{video_data_table};\n",
    "                \"\"\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        v_etc = pd.DataFrame(cursor.fetchall(), columns=columns)\n",
    "        if len(v_etc) == 0:\n",
    "            now_limit += 1\n",
    "            time_date += 1\n",
    "            continue\n",
    "        ########################################################\n",
    "\n",
    "        # 1번 13번 클러스터 제외\n",
    "        v_etc = v_etc[~v_etc.video_cluster.isin([1, 13, \"None\"])]\n",
    "        v_etc['video_published'] = pd.to_datetime(v_etc['video_published'])  # video_published 컬럼을 datetime 타입으로 변환\n",
    "        v_etc.dropna(subset=['use_text'], inplace=True)\n",
    "        \n",
    "        # print(start_date_str,\"~\",end_date_str, \" 시작\")\n",
    "        columns = [\"video_id\", \"video_views\", \"video_performance\", \"YEAR\", \"MONTH\", \"DAY\"]\n",
    "        columns_to_str = \", \".join(columns)\n",
    "\n",
    "        ########################### 히스토리 가져오기 to temp\n",
    "        etc = pd.DataFrame()\n",
    "        h_etc = pd.DataFrame()\n",
    "        for date in dates:\n",
    "            history_data_table = f\"video_history_{date}\"\n",
    "            query = f\"\"\"\n",
    "                        select {columns_to_str} from dothis_temp.{history_data_table};\n",
    "                    \"\"\"\n",
    "            cursor.execute(query)\n",
    "            _h_etc = pd.DataFrame(cursor.fetchall(), columns=columns)\n",
    "            h_etc = pd.concat([h_etc, _h_etc], axis=0)\n",
    "            \n",
    "        etc = pd.merge(v_etc, h_etc, on='video_id', how='inner')            \n",
    "        etc = etc.drop_duplicates()\n",
    "        # print(\"len(etc)\", len(etc)) \n",
    "\n",
    "        df = pd.concat([df, etc], axis=0)\n",
    "        time_date += 1\n",
    "        now_total  += 1\n",
    "        print(f\"Use {start_date_str} ({now_total}/{end_total})\")\n",
    "        # logger.info(f\"Use {start_date_str} ({now_total}/{end_total})\")\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "    ############################ 조회수 계산하기\n",
    "    # crawled_date를 datetime 형식으로 변환\n",
    "    # null 값을 0으로 대체 및 소수점 이하 제거 및 정수형 변환\n",
    "    df[[\"YEAR\", \"MONTH\", \"DAY\"]] = df[[\"YEAR\", \"MONTH\", \"DAY\"]].fillna(0).astype(int)\n",
    "    df[\"crawled_date\"] = pd.to_datetime(df['YEAR'].astype(str)+\"-\"+df['MONTH'].astype(str)+\"-\"+df['DAY'].astype(str), format=\"%Y-%m-%d\", errors='coerce')\n",
    "    df.sort_values(['crawled_date'], ascending=True, inplace=True)            \n",
    "    df.reset_index(drop=True, inplace=True)   \n",
    "    \n",
    "    # 각 id에 대해 최신 날짜를 가진 행의 인덱스를 찾기\n",
    "    idx = df.groupby('video_id')['crawled_date'].idxmax()\n",
    "\n",
    "    # 인덱스를 이용해 최신 날짜 데이터를 필터링\n",
    "    df = df.loc[idx]\n",
    "    df = df.drop_duplicates()\n",
    "    df.use_text = df.use_text.progress_apply(decode_and_convert)\n",
    "    df.use_text = df.use_text.progress_apply(lambda x: \" \".join([i[0] for i in x]))\n",
    "\n",
    "    df.sort_values(['crawled_date'], ascending=True, inplace=True)            \n",
    "    df.reset_index(drop=True, inplace=True)          \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 2024-08-31 (1/7)\n",
      "Use 2024-08-30 (2/7)\n",
      "Use 2024-08-29 (3/7)\n",
      "Use 2024-08-28 (4/7)\n",
      "Use 2024-08-27 (5/7)\n",
      "Use 2024-08-26 (6/7)\n",
      "Use 2024-08-25 (7/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42611 [00:00<?, ?it/s]\r  3%|▎         | 1083/42611 [00:00<00:03, 10804.64it/s]\r  5%|▌         | 2164/42611 [00:00<00:06, 5958.97it/s] \r  8%|▊         | 3261/42611 [00:00<00:05, 7543.41it/s]\r 10%|█         | 4377/42611 [00:00<00:04, 8668.83it/s]\r 13%|█▎        | 5355/42611 [00:00<00:05, 6660.25it/s]\r 15%|█▌        | 6400/42611 [00:00<00:04, 7583.75it/s]\r 18%|█▊        | 7510/42611 [00:00<00:04, 8495.42it/s]\r 20%|██        | 8603/42611 [00:01<00:03, 9152.60it/s]\r 23%|██▎       | 9600/42611 [00:01<00:04, 6744.29it/s]\r 25%|██▍       | 10610/42611 [00:01<00:04, 7490.13it/s]\r 28%|██▊       | 11770/42611 [00:01<00:03, 8477.18it/s]\r 30%|███       | 12939/42611 [00:01<00:03, 9299.55it/s]\r 33%|███▎      | 14098/42611 [00:01<00:02, 9911.73it/s]\r 36%|███▌      | 15164/42611 [00:01<00:04, 6788.53it/s]\r 38%|███▊      | 16270/42611 [00:02<00:03, 7683.83it/s]\r 41%|████      | 17405/42611 [00:02<00:02, 8528.36it/s]\r 43%|████▎     | 18500/42611 [00:02<00:02, 9126.43it/s]\r 46%|████▌     | 19610/42611 [00:02<00:02, 9641.42it/s]\r 49%|████▊     | 20743/42611 [00:02<00:02, 10094.92it/s]\r 51%|█████     | 21818/42611 [00:02<00:03, 6519.55it/s] \r 54%|█████▍    | 22971/42611 [00:02<00:02, 7531.83it/s]\r 57%|█████▋    | 24106/42611 [00:02<00:02, 8384.05it/s]\r 59%|█████▉    | 25246/42611 [00:03<00:01, 9115.05it/s]\r 62%|██████▏   | 26374/42611 [00:03<00:01, 9671.20it/s]\r 64%|██████▍   | 27472/42611 [00:03<00:01, 10022.41it/s]\r 67%|██████▋   | 28610/42611 [00:03<00:01, 10398.64it/s]\r 70%|██████▉   | 29706/42611 [00:03<00:02, 6312.94it/s] \r 72%|███████▏  | 30882/42611 [00:03<00:01, 7375.70it/s]\r 75%|███████▌  | 32022/42611 [00:03<00:01, 8245.53it/s]\r 78%|███████▊  | 33182/42611 [00:04<00:01, 9040.55it/s]\r 80%|████████  | 34300/42611 [00:04<00:00, 9582.13it/s]\r 83%|████████▎ | 35382/42611 [00:04<00:00, 9907.23it/s]\r 86%|████████▌ | 36531/42611 [00:04<00:00, 10341.16it/s]\r 88%|████████▊ | 37682/42611 [00:04<00:00, 10670.01it/s]\r 91%|█████████ | 38796/42611 [00:04<00:00, 5945.15it/s] \r 94%|█████████▎| 39938/42611 [00:04<00:00, 6952.92it/s]\r 96%|█████████▋| 41073/42611 [00:05<00:00, 7868.19it/s]\r 99%|█████████▉| 42239/42611 [00:05<00:00, 8737.54it/s]\r100%|██████████| 42611/42611 [00:05<00:00, 8301.86it/s]\n",
      "\r  0%|          | 0/42611 [00:00<?, ?it/s]\r 93%|█████████▎| 39430/42611 [00:00<00:00, 394296.62it/s]\r100%|██████████| 42611/42611 [00:00<00:00, 392370.29it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'download_file_from_s3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(local_dir)\n\u001b[1;32m      8\u001b[0m df \u001b[39m=\u001b[39m vbr_data_collect()\n\u001b[0;32m---> 10\u001b[0m download_file_from_s3(bucket_name\u001b[39m=\u001b[39mbucket_name,\n\u001b[1;32m     11\u001b[0m                       s3_prefix\u001b[39m=\u001b[39ms3_prefix,\n\u001b[1;32m     12\u001b[0m                       local_dir\u001b[39m=\u001b[39mlocal_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_file_from_s3' is not defined"
     ]
    }
   ],
   "source": [
    "bucket_name = \"dothis-ai\"\n",
    "s3_prefix = \"models/related/\"\n",
    "# local_dir = f\"/app/{s3_prefix}\"\n",
    "local_dir = \"../../../testdata\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "    \n",
    "df = vbr_data_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping directory ../../../testdata/\n",
      "Downloading models/related/related_model.bin to ../../../testdata/related_model.bin\n",
      "Downloaded models/related/related_model.bin to ../../../testdata/related_model.bin\n",
      "Downloading models/related/related_model.bin.syn1neg.npy to ../../../testdata/related_model.bin.syn1neg.npy\n",
      "Downloaded models/related/related_model.bin.syn1neg.npy to ../../../testdata/related_model.bin.syn1neg.npy\n",
      "Downloading models/related/related_model.bin.trainables.syn1neg.npy to ../../../testdata/related_model.bin.trainables.syn1neg.npy\n",
      "Downloaded models/related/related_model.bin.trainables.syn1neg.npy to ../../../testdata/related_model.bin.trainables.syn1neg.npy\n",
      "Downloading models/related/related_model.bin.wv.vectors.npy to ../../../testdata/related_model.bin.wv.vectors.npy\n",
      "Downloaded models/related/related_model.bin.wv.vectors.npy to ../../../testdata/related_model.bin.wv.vectors.npy\n",
      "Downloading models/related/related_model.bin.wv.vectors_ngrams.npy to ../../../testdata/related_model.bin.wv.vectors_ngrams.npy\n",
      "Downloaded models/related/related_model.bin.wv.vectors_ngrams.npy to ../../../testdata/related_model.bin.wv.vectors_ngrams.npy\n",
      "Downloading models/related/related_model.bin.wv.vectors_vocab.npy to ../../../testdata/related_model.bin.wv.vectors_vocab.npy\n",
      "Downloaded models/related/related_model.bin.wv.vectors_vocab.npy to ../../../testdata/related_model.bin.wv.vectors_vocab.npy\n"
     ]
    }
   ],
   "source": [
    "download_file_from_s3(bucket_name=bucket_name,\n",
    "                      s3_prefix=s3_prefix,\n",
    "                      local_dir=local_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Josa List:   0%|          | 0/205 [00:00<?, ?it/s]\rReading Josa List: 100%|██████████| 205/205 [00:00<00:00, 2165824.48it/s]\n",
      "\rReading Stopwords List:   0%|          | 0/1907 [00:00<?, ?it/s]\rReading Stopwords List: 100%|██████████| 1907/1907 [00:00<00:00, 3247477.76it/s]\n",
      "\rReading Josa List:   0%|          | 0/205 [00:00<?, ?it/s]\rReading Josa List: 100%|██████████| 205/205 [00:00<00:00, 2323871.14it/s]\n",
      "\rReading Stopwords List:   0%|          | 0/1907 [00:00<?, ?it/s]\rReading Stopwords List: 100%|██████████| 1907/1907 [00:00<00:00, 3310653.03it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../../../usedata\"\n",
    "gr_path = os.path.join(local_dir, \"related_model.bin\")\n",
    "josa_path = os.path.join(data_path, \"kor_josa.txt\")\n",
    "stopwords_path = os.path.join(data_path, \"stopwords_for_related.txt\")\n",
    "model = \"word2vec\"\n",
    "\n",
    "gr = GensimRelated(path=gr_path,\n",
    "                   josa_path=josa_path,\n",
    "                   stopwords_path=stopwords_path,\n",
    "                   model=model)\n",
    "\n",
    "vr = VBR(df,\n",
    "        josa_path=josa_path,\n",
    "        stopwords_path=stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_score_result(word, vbr_ratio=0.55, inference_ratio=0.45):\n",
    "    related_inference = gr.gensim_related(word, split_word_check=True)\n",
    "    vbr_related = vr.vbr_related(word, ranking=1, threadholds=5, df_len=10000)\n",
    "\n",
    "    if (len(related_inference) > 0) and (len(vbr_related) > 0):\n",
    "    \n",
    "        vbr_score_scales = [(x / max(vbr_related.values())) * vbr_ratio for x in vbr_related.values()]\n",
    "        inference_score_scales = [(x / max(related_inference.values())) * inference_ratio for x in related_inference.values()]\n",
    "        # 업데이트\n",
    "        vbr_related = dict(zip(vbr_related.keys(), vbr_score_scales))\n",
    "        related_inference = dict(zip(related_inference.keys(), inference_score_scales))\n",
    "        # 두 딕셔너리의 모든 연관어 모음\n",
    "        all_related_word = list(set(vbr_related.keys()) | set(related_inference.keys()))\n",
    "        \n",
    "        ### dict.get(key, default)로 해당 연관어가 없으면 0이 반환되도록 함.\n",
    "        combined_dict = dict()\n",
    "        for related_word in all_related_word:\n",
    "            vbr_score_value = vbr_related.get(related_word, 0)\n",
    "            inference_score_value = related_inference.get(related_word, 0)\n",
    "            \n",
    "            combined_score = vbr_score_value + inference_score_value\n",
    "            ### combined_score가 null값이거나 0이 아닌 경우에만 저장\n",
    "            if (not math.isnan(combined_score)) and (combined_score != 0):\n",
    "                combined_dict[related_word] = combined_score\n",
    "        # 값을 기준으로 내림차순 정렬\n",
    "        combined_dict = dict(sorted(combined_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    elif (len(related_inference) > 0) and (len(vbr_related) == 0):                    \n",
    "        inference_score_scales = [(x / max(related_inference.values())) * inference_ratio for x in related_inference.values()]\n",
    "        related_inference = dict(zip(related_inference.keys(), inference_score_scales))\n",
    "        # 값을 기준으로 내림차순 정렬\n",
    "        combined_dict = dict(sorted(related_inference.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "    elif (len(related_inference) == 0) and (len(vbr_related) > 0):                    \n",
    "        vbr_score_scales = [(x / max(vbr_related.values())) * vbr_ratio for x in vbr_related.values()]\n",
    "        vbr_related = dict(zip(vbr_related.keys(), vbr_score_scales))\n",
    "        # 값을 기준으로 내림차순 정렬\n",
    "        combined_dict = dict(sorted(vbr_related.items(), key=lambda item: item[1], reverse=True))\n",
    "    else:\n",
    "        combined_dict = dict()\n",
    "    return combined_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'웨스트': 0.9085145628561144, '내한': 0.8386440612935728, '리스닝파티': 0.6599890596351143, '리스닝': 0.4846560222636259, '파티': 0.4846560222636259, 'west': 0.4774462866441829, '카니예': 0.45, '카녜': 0.40383379188127966, '런어웨이': 0.4027723910652595, 'Kanye': 0.3841926178140266, 'kanye': 0.3406707797317701, '컨버스': 0.2896510351122901}\n"
     ]
    }
   ],
   "source": [
    "word = \"칸예\"\n",
    "print(scale_score_result(word, vbr_ratio=0.55, inference_ratio=0.45))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suchoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b79ff75884a38b2c2e2e135fb035189518295fed6dfd1b18789ebc978ed92a17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
